==========
Scheduling
==========

1) Strategies
-------------

Different strategies exists:
  - Take into account priority
  - Take into account responsiveness
  - Take into account fairness

Priority is related to the nice() syscall, higher nice number is associated to
lower priority.

There is also real time priority, 0-99 values and this time higer number is an
higher priority.

In task_struct the fields related to priority are:

static_prio, priority as seen from userspace.
normal_priority, tasks with same static_prio will get different normal.
prio, dynamic priority associated by kernel.
rt_prio, real time priority.

Nice values are multiplicative, going one level up is assigning -10% of the CPU
usage.
Priority values are mapped to time slices using weight.
See the sched_prio_to_weight[40] array.

2) Classes
----------

SCHED_FIFO is a realtime FIFO scheduler in which each process yield the cpu.
SCHED_RR realtime round robin.
SCHED_OTHER/SCHED_NORMAL common round robin-shared policy.
SCHED_DEADLINE Constant Bandwidth Server (CBS) on top of Earliest Deadline First
queues.
SCHED_DEADLINE CBS replaced with Greedy Reclamation of Unused Badwidth (GRUB).

The load of the CPU are managed by the idle process and used by the power
management.

3) Structures
-------------

A run queue is a fundamental datat structure for the scheduler. It describe 
waht tasks can be scheduled on a CPU.

struct rq {
  uint nr_running // number of tasks that can be runned in this CPU
  ulong cpu_load[]
  ...
  tasklet curr
}

One rq for each CPU. Retrieve it with cpu_rq(cpu), this_rq(), task_rq(p),
cpu_curr(cpu).

A wait queue keeps tracks of process that cannot be scheduled, waiting for I/O
for example.

Enque in the wait queue is a way to put in sleep in kernel space.
Suffered many performance problems as the thundering herd effect (waking up many
tasks competing for a resource and then put again the majority in wait).

A wait queue entry can have the exclusive flag that allow only a process at time
to wake up to solve this problem.
add_wait_queue() is used to enqueue without exclusive flag,
add_wait_queue_exclusive() to use it.

All tasks with the exclusive flag are at the end of the list to ensure that
there is not interleaving between the two types of entries.

4) Entry point
--------------

schedule() is the scheduler entry point and is composed by 3 phases:

  - checks on current (e.g. pending signals)
  - selection of next process
  - context switch

This function is called in several places by direct invocation o using a lazy
invocation using need_resched givin an hint that the scheduler should be
activated soon.

schedule_tick() is called from update_process_times(). This determinates if the
time quantum expired and sets need_resched.

5) States
---------

The state of a process is kept in the PCB. The possible values are:

 - TASK_RUNNING
 - TASK_ZOMBIE
 - TASK_STOPPED
 - TASK_INTERRUPTIBLE (resumed due to an interrupt, return -EINTR)
 - TASK_UNINTERRUPTIBLE
 - TASK_KILLABLE

Wait and runnign state are the same in Linux and mapped in TASK_RUNNING.

6) Schedulers
-------------

O(n) Scheduler iterates all tasks with time divided in epochs. When an epoch
ends, al least each running task has run once. If a process didn't use the
entire quantum the remaining is added to the next time slice.

Disadvantages: Also a non-runnable task is searched for goodness, mixture of
runnable/non-runnable tasks in a runque, not thread safe (schedule() under lock
other cores waits).

Advantages: Perfect load sharing (single runqueue for all cores), no CPU
underutilization.

The runqueue here is a list.

O(1) Scheduler, introduces the global priority scale (the weights).
It uses early preemtion, when a task enter in TASK_RUNNING its priority is
checked to see if schedule() must be called.

Runqueue here is 2 arrays of lists (a list for each priority level), active and
expired. To conclude an epoch just exchange the pointers to those arrays.
schedule_find_first_set scan a bitmap and take first task from non-empty queue.
When swapping array midy this bitmap.

There is data separation, each core has its own queue.

Cross CPU scheduling allows a core to see other core queue and do come load
balancing, this must be protected by locks. Tipically done by CPU each 200 ms,
each 1ms when the CPU is in idle.

Staircase scheduler, by Kolivar to increase the "responsiveness" of the O(1)
scheduler on low resource environment (< ~10 cores). It mostly drop the
priority recalculation using a simple rank scheme.

Favors interactive processes rather CPU bound ones. 

Complete fair scheduler (default from 2007) uses a red-black tree where nodes
are ordered by execution time (nanoseconds, HRT). 

7) Context switch
-----------------

switch_to(next, prev, last) macro is machine dependant. In x86 mainly:
 - change TSS
 - update CPU control registers

It preload in page table some info to avoid page faults in assembly code
(otherwise no fixup) and then call switch_to_asm.
