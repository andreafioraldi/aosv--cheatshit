===========
Concurrency
===========

1) Properties
-------------

 - Safety: nothing wrong happens
    * we can associate with a correct sequential execution
 - Liveness: eventually something good happens
    * no starvation

The linearizability property tries to generalize the intuition of safety.
History is a sequence of invocation and replies on an object by multiple
threads. A sequential history is when invocation has immediate responses.

A history is linearizable if the invocations and responses can be reordered
to produce a sequential history. A response that precede an invocation must
also maintain such order in the reordered history.

An object is linearizable if every valid history associated can be linearized.

Liveness (Progress) conditions:

 - Deadlock-free: no deadlock but possibly wasting time
 - Starvation-free: no one thread get stuck
 - Lock-free: some method call completes
 - Wait-free: every method call completes (very rare)
 - Obstruction-free: every method calls complete if executed in isolation,
   if you update data that no other thread is touching it will be completed.
   This is in-between wait and lock.

CAS is lock free cause can fail and not update the memory location.

                   Non-Blocking             |    Blocking
-------------------------------------------------------------
For everyone | wait-free | obstruction-free | starvation-free
-------------------------------------------------------------
For some     | lock-free |                  | deadlock-free


For some is problematic, is not strictly related to my code but to the
offered scheduling.

2) Concurrent and preemptive kernels
------------------------------------

Everything in the OS is a task. A modern OS is also concurrent.

The kernel must ensure consistency and avoid deadlocks.

Typical solutions are:

 - Explicit synchronization
 - Non-blocking synchronization
 - Data separation (per-CPU variables)
 - Interrupt disabling
 - Preemption disabling

3) Race condition example
-------------------------

my_syscall() {
  n = len(queue)
  if (n > 0) {
    m = kmalloc(1024)
    pop(queue)
  }
}

0. n == 1
1. kmalloc() stops, thread got to sleep
2. control to another task, call to my_syscall again
3. n is again 1
4. 2 pops of a queue with len = 1

4) Enable/disable
-----------------

preemt_disable() increments per-CPU counter.
preemt_enable() decrements per-CPU counter.
preemt_count() reads this counter.

Non 0 counter tells that preemption is disabled.

Without counter is not possible to call a routine that disables and then enable
preemption inside a routing that disables preemption, call the previous
function, then enable preemption.

For each CPU we can reset the IF flag with:

local_irq_disable()
local_irq_enable()
irq_disabled()

Nested activations done with:

local_irq_save(flags)     [pushf + pop flags]
local_irq_restore(flags)  [push flags + popf]

get and put macros of per-CPU vars disable and enable preemption for us.
per-CPU variables and rescheduling on another core are not good friends.

5) Atomic operations
--------------------

atomic_t type and atomic_fetch_{add,sub,...} implements RMW instructions.

Bitmap macros are RMW. set_bit(), clear_bit(), ...

6) Barriers
-----------

Compiler can reorder instruction, avoid with optimization barrier:

#define barrier() asm volatile ("":::"memory")

Out of order pipeline can reorder memory access, use memory barriers:

  - {smp_}mb(): full memory barrier
  - {smp_}rmb(): reads mem barrier
  - {smp_}wm(): writes mem barrier

7) Mutex and spinlocks
----------------------

The terminology is that down and up are wait and signal.

Spinlock does not cause sleep but are polling of a variable. This is good when
the critical section is small and sleep/wake up time is more than the wait time.

spin_lock_irq and spin_lock_irqsave.
We do not want interrupts in the middle of a critical section, this will
increase the time. 

Any spinlock will disable preemption. spin_lock_irq calls local_irq_disable,
spin_lock_irqsave calls local_irq_save.

With spinlocks, only one thread at a time can be in the critical section.

For this use read/write locks. Multiple readers, one writer.
Cannot both read and write, with many readers writers can starve.
Concurrent execution of read operations.

To avoid to starve writers on small simple data (no pointers and no side
effects) we can use seqlocks.

write_seqlock increments, write_sequnlock also increments.
Enter in the critical section when the counter is even.

Readers do not acquire the locks, they retry to read data.

do {
  seq = read_seqbegin(&lock);
  /* make A COPY */
} while (read_seqretry(&lock, seq));

8) Read-Copy-Update
-------------------

RCU is Lock-free. Many readers and one writer concurrently.

3 fundamental mechanism:
  - publish-subscribe for insertion
  - wait for pre-existing readers when deleting
  - maintain multiple versions of RCU-updated objects for readers

Only dynamically allocated data structures, the code can't sleep inside
an RCU critical section!

Use rcu_assign_pointer as publish to not reorder with barriers.

Read:

rcu_read_lock(); //no real lock, disable preemtion
p = rcu_dereference(gp); //mem berriers
if (p != NULL) do_something_with(p->a, p->b);
rcu_read_unlock();

syncronize_rcu() asks the OS to schedule the code of delete operation on all
CPU cores.
Cannot run an operation on a core that is reading cause preemption is disabled.
This wait for all pre-existing readers to complete.

list_del_rcu(&p->list); // unlink
synchonize_rcu(); // wait for readers reading the victim node
kfree(p); // release memory, no UAF here :)

When updating the scenario is similar.

Make a copy of the node, modify the node, list_replace_rcu, sync, kfree(old).
