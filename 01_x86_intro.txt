======================
An Introduction to x86
======================

1) Branch prediction
--------------------

Conditional branches are around 20% of the instructions in the code.

 - Dynamic branch prediction, implemented in hardware and based on history
 
 - Static branch prediction is commonly user-assisted and determined by
   the compiler (likely/unlikely)

The branch prediction table is a small memory indexed by the lower bits of the
conditional instruction address.

 - Prediction "take" is correct: 1 cycle penalty
 - Predition "not take" is correct: no penalty
 - Uncorrect prediction: change prediction, flush pipeline

The 2 bit saturating counter is a 4 state table. T-T-NT-NT.

A correlated predictor try to solve the problem of predictions that depends on
multiple branches.
It keeps a history of past m braches.

A tournament predictor uses different predictor and associate, for each branch,
the best predictor using 2-bit.

For indirect branches, the game is harder.
The branch target buffer is used (a small cache) when prediction bits are
coupled with target addresses.

For ret instruction, there is the return address stack buffer.

The fetch both targets technique is a possibility but does not help with
multiple targets (e.g. switches).

2) Hyper-threading
------------------

A copy of the architectural state among each logical core. Less hardware
required but needs an arbitration mechanism.

In the Intel case there is a division in the architecture:

 - Front-end:

   This stage delivers instructions to the later pipeline. They are not
   x86 CISC instructions but RISC u-ops that are converted by the microcode.
   u-ops are cached in the Execution Trace Cache (TC).
   
   TC entries are tagged with thread information and the access is arbitrated
   each CPU cycle. TC replace entries in LRU.
   
   TC sends requests to get new instructions to the Instruction Translation
   Lookaside Buffer (ITLB)
 
 - Out-of-order Engine:

   The u-op queue decoupled the frontend from this part. The original program
   order is not taken into account. The allocator group u-op based on their
   features (e.g. buffer for load/store, another for int arith).
   
   The register renaming allows u-ops of different threads to operate in data
   separation. It also put u-ops in two new queues, one for memory operations
   another for other operations. 
   
   Instruction schedulers take u-ops from the 2 queues and are thread-oblivious.
   u-ops are evaluated only in base of the avaiability of their operands.
   
   The retirement stage commits the microarchitectural stage to the
   architectural stage. Store operations here are written to L1 cache.

3) Multi-core
-------------

The Cache Coherence defines the correct behavior of the caches regardless of how
they are employed.

 - Strong Coherence:
    
   All memory operation to a single location (coming from all
   cores) must be executed in a total order that respects the order of the
   commits of each core.
   
   Two invariants used to implements: SWMR, only one core that can R/W or many
   cores that can only read a single location. DV says that when an epoch
   starts the value of a location must be the same value that was at the end
   of the last epoch with a single R/W core.
 
 - Weak Coherence:
 
   For performance, SWMR is dropped and so DV is only eventually. Loads see the
   last written value only after N epochs.
  
 - No Coherence:
 
   The fastest. Programmers must coordinate cache with explicit requests.

There are two kinds of coherency transactions:

  - Get: load a block into a cache line
  - Put: evict the block into a cache line

These transactions are associated with messages exchanged to ensure the chosen
coherence model.

There are families fo coherence protocols:

 - Invalidate protocols: when a core writes it invalidates all other copies in
   the other cores.
 
 - Update protocols: when a core writes to its cache it also writes to the other
   caches in all the other cores.
 
 - Snooping Cache: coherence requests are broadcasted to all cores. It needs
   arbitrarization on the bus to serialize requests and an interconnection layer
   for the total order.
   Fast but not scalable.
 
 - Directory Based: coherence requests are unicsated to a directory that
   forwards only to the appropriate cores. Directory does also serialization.
   Scalable but not fast.

A cache controller is behind each private caches of each core.

A protocol should offers:

 - Validity: a valid block has the most up-to-date value. Can be written only
   if it is also exclusive.
   
 - Dirtiness: a dirty block has the most up-to-date value but it differs from
   the values stored in RAM.
   
 - Exclusivity: an exclusive block is the only copy of the block across all
   caches.
   
 - Ownership: a cache controller is the owner of a block if it is responsible
   for its coherence requests.

Write-Through caches have no dirty block cause all stores are immediately
propagated to the last level cache (LLC).

False Cache Sharing is when two cores access to different data items that are
on the same cache line. It produces an un-needed invalidation.

The L1 cache is indexed using virtual address and avoid a TLB lookup. Other
caches use physical addresses.

The Virtual Aliasing problem occurs when there is a cache indexed with virtual
addresses.

One type is the homonyms problem cause a virtual address can be associated to
multiple physiscal addresses. A tag may not identify a unique data.
This can be avoided tagging with an Address-Space-ID or flushing cache on
context switch.

Another type the synonyms problem cause several virtual addresses may be mapped
to the same physical address. On cache write only one of them is updated.
ASID here are not useful, the cache should be flushed on context switch or
a hardware system to detect the problem or restricting the mapping so that
shared memories maps on the same cache line.
