=================
Memory management
=================

1) Numa nodes
-------------

Numa nodes are organized in pglist_data structs, pg_data_t is the typedef.
pddat_list is the list of structs describing Numa nodes.

Physical memory is managed in term of page frame numbers.
The index in the list implicitly gives us the physical address.

In each pg_data_t there is an array of associated zones.

ZONE_NORMAL are the first ~ 900 MB for the kernel.
What is ZONE_NORMAL is stable and always in the page table.

ZONE_HIGHMEM mapping is transient (e.g. userspace mem)

ZONE_DMA mainly for buffer caching (e.g. write back to hard disk)

2) Zones intialization
----------------------

The zones are initialized after the page table.
The e820 table in memory is filled by BIOS and describes what page frames are
available.

zone_t {
  free_area_t* free_area
  page* zone_mem_map
  ...
}

page {
  list of mappings
  count (virtual references to the frame, think about cow)
  flags
}

Stady state allocator must be setupped in a way to not touch bootmem allocated
memory (marked as PG_reserved in page.flags).
At the end of the initialization, this memory will be given back.

Each description of Numa node is in the memory associated with the node
(locality for performance).

The buddy allocator in general: two buddies (chunks) can be consolidated with
size power of 2.
It can be viewed as a binary tree with labels allocated/free
(leaves represent frames).
e.g. we want to allocate 2*PAGE_SIZE, we search until there is a last level node
with the two leafs both free.

free_area_t {
    list { prev, next }
    uint *map
}

The implementation of Buddy is Linux is not based on a binary tree. There is a
free list and an array to access in O(1) a level. Each level has a list of
blocks. When two blocks are consolidated they are moved to the higher level.

In free_area_t we have a bitmap in which each bit represents two buddies that
is a block that can be served to a request by a higher level.
When one bit is 1 means that it is split and so at a lower level.
Freeing is simply setting a bit.
To request a block request we traverse starting from 0 to MAX_ORDER.

!!! We need sync for this, in fact, in zone_t there is a spinlock.

3) High memory
--------------

This is not a permanent mapping.

The API to retrieve memory from HIGHMEM are different respect the API used to
retrieve memory from NORMAL.

Allocation:

 - vmap() get long duration mapping.
 - kmap() short duration mapping of single page
 - kmap_atomic() like kmap but for only the cpu that issued it.

Deallocation:

An array of counter pkmap_count[LAST_PKMAP] increments the correct entry when
the associated page is mapped.

 - 0 not mapped
 - 1 not mapped now but cached (we mast then invalidate TLB)
 - n > 1 mapped n-1 times

kunmap(page) decrement this reference counter.

4) Reclaiming boot memory
-------------------------

mem_init() destroy the bootmem allocator.

The frames are released resetting PG_RESERVED bit.

A fake buddy free is issued (set count to 1 and then __free_page) to put such
memory into the buddy system.

5) Allocation context
---------------------

In a process context, if the request cannot be served wait, there is a priority
based approach. In an interrupt context do not wait.

linux/malloc.h exposed memory management API to other kernel subsystems.

get_zeroed_page, free_page, etc...

There are no sanity checks in free_page (fake chunks yeeee).

There are flags are used to specify the allocation context.

 - GFP_ATOMIC interrupt context (critical, cannot lead to sleep)

 - GFP_USER allocate mem for userspace activities (but not directly used in
   userspace) (can lead to sleep)

 - GFP_BUFFER alloc a buffer (can lead to sleep)

 - GFP_KERNEL alloc kern mem (can lead to sleep)

With alloc_pages_node we can specify NUMA policies.
On the contrary, __get_free_pages is agnostic of NUMA.

(TODO in userspace see numa_move_pages)

On top of Buddy system, the kernel builds a set of fast allocators.
For example pdg_alloc and al. to allocate page tables.

Other allocators are Quicklists and slab/slub/slob allocators.

Quicklists are lists of pages living on a single core.

Note: get_cpu_var(var) like TLS and also disable preemtion
      put_cpu_var(var) enable again preemption (scheduling to another thread)
      after processing per CPU var

quicklist {
    void* page
    int nr_pages 
}

page points to pages that have headers to be a linked list of pages.
nr_pages is the size of such a list.

quicklist_alloc get a page from Quicklist (there are many for a core accessible
with an index nr).
When updating selected Quicklist this is done under get_cpu_var.

If there are not available pages in Quicklist fallback to __get_free_page.

Another flag for Buddy system: __GFP_ZERO like calloc.

6) The SLAB allocator
---------------------

It works on top of the buddy system.

The allocator has caches that have 3 buckets (full/partial/free).
Each slab manages a set of pages (continuous in physical memory due to the buddy
structure).
In each page, there are many objects.

cache:
--> slabs_full:
----> slabs:
------> pages:
--------> object
--------> ...
--------> object
--> slabs_partial:
...
--> slabs_free:
...

Each slab manages objects of the same size. Slab are moved to full/partial/free
based on the state of objects in pages.

The slab interface is kmalloc(size, flags), kfree(addr),
kmalloc_node(size, flags, node).

Slab coloring:

page: | object | coloring | object | coloring | ... |

len ( | object | coloring | ) = L1_CACHE_BYTES

Coloring is padding to ensure that an object is not split in L1 cache
(so we avoid cache miss when traversing an entire object)

So objects are L1 cache aligned. 
